---
title: "test Ames"
date: "8/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
***

This is a test on the Ames data set, using different machine learning models.

## Libraries
***

```{r libraries, message=FALSE, warning=FALSE}
library(AmesHousing)
library(ggplot2)
library(tidymodels)
library(dplyr)
library(patchwork)
library(kableExtra)
```

## Functions
***

```{r functions}

tidy_tbl <- function(fit_obj) {
    
    data_tbl <- fit_obj %>%
        broom::tidy()
  
    

    table <- data_tbl %>%
        kbl() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                      full_width = FALSE,
                      position = 'left') %>%
        column_spec(5, color = 'white',
                    background = spec_color(data_tbl$p.value,
                                            begin = .2, end = .7,
                                            option = 'D',
                                            direction = -1))
  
    return(table)
  
}


## Function to plot the frequency of nominal variables
nom_bar_plot <- function(dataset, col, title) {
  
  col <- sym(col)
  
  ggplot(dataset, aes(y = forcats::fct_infreq(!!col))) +
      geom_bar(fill = 'steelblue', alpha = .5) +
      labs(title = title, x = '', y = '') +
      theme_minimal()

}

```

## Data
***

All residential home sales in Ames, Iowa between 2006 and 2010. The data set contains many explanatory variables on the quality and quantity of physical attributes of residential homes in Iowa sold between 2006 and 2010. Most of the variables describe information a typical home buyer would like to know about a property (square footage, number of bedrooms and bathrooms, size of lot, etc.). A detailed discussion of variables can be found in the original paper referenced below.

*De Cock D. 2011. Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education; 19(3)*

The data is loaded into the package `AmesHousing`; it allows the user to work on the raw data (`ames_raw`) or on a pre-processed data set (`make_ames`), which is the one used in this example.

The raw data set is comprised of 82 fields (*variables*) recorded for 2.930 properties in Ames IA (*observations*).

For the processed version a summary of the differences between these data sets and `ames_raw` is:

* All factors are unordered.  
* PID and Order are removed.  
* Spaces and special characters in column names where changed to snake case. To be consistent, SalePrice was changed to Sale_Price.  
* Many factor levels were changed to be more understandable (e.g. Split_or_Multilevel instead of 080)  
* Many missing values were reset. For example, if the variable Bsmt_Qual was  missing, this implies that there is no basement on the property. Instead of a  missing value, the value of Bsmt_Qual was changed to No_Basement. Similarly,  numeric data pertaining to basements were set to zero where appropriate such as  variables Bsmt_Full_Bath and Total_Bsmt_SF.  
* Garage_Yr_Blt contained many missing data and was removed.  
* Approximate longitude and latitude are included for the properties. Also, note  that there are 6 properties with identical geotags. These are units within the same building. For some properties, updated versions of the PID identifiers were found  and are replaced with new values.

```{r}
ames <- make_ames()
```

## Data Partition
***

```{r data_partition, message=FALSE}
# set seed for reproducibility
set.seed(123)

# main split (stratified Sale_Price)
ames_split <- initial_split(ames, strata = Sale_Price, prop = .7)

# train and test data sets
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)

# set seed for reproducibility
set.seed(456)
# cross-validation data set (n = 10) from train 
ames_folds <- vfold_cv(ames_train, v = 10, strata = Sale_Price)
```


## EDA
***

Some basic EDA on the training data set.

```{r}
skimr::skim(ames_train)
```

```{r}
glimpse(ames_train)
```

```{r}
head(ames_train)
```

### Target Variable

The **target variable** is `Sale_Price`. Below its distribution.

```{r, message=FALSE, warning=FALSE, fig.align='center', out.width='80%'}
px_dist <- ames_train %>%
    select(Sale_Price) %>%
    ggplot(aes(x = Sale_Price)) +
    geom_histogram(bins = 60, alpha = .75, fill = 'steelblue') +
    geom_vline(xintercept = median(ames$Sale_Price),
               color = 'red', lty = 'dashed') +
    geom_vline(xintercept = mean(ames$Sale_Price),
               color = 'darkgreen', lty = 'solid') +
    scale_x_continuous(labels = scales::label_number(),
                       breaks = scales::breaks_pretty(n = 7),
                       expand = c(0.01, 0.01)) +
    labs(title = 'Sale Prices') +
    theme_minimal()

px_dist
```

The histogram shows that the most common selling level to be around \$160'000 (*median*) and the average selling price to be \$180'000 (*mean*). The plot shows also a right skewed distribution which suggest there are concern with an assumption of normality. To solve this problem it could be better to log-transform the variable.  

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.align='center'}
px_log_dist <- px_dist +
    scale_x_log10(labels = scales::label_number(),
                  expand = c(0.01, 0.01)) +
    labs(title = 'Log-Sale Prices')

px_dist | px_log_dist
```

The log-transformation helps to meet normality assumption, highlighting a couple of interesting information. The log-transformed distribution appears to be slightly multimodal and it helps us to see a couple of outliers, with one observation where the `Sale_Price` is close to zero.

Next check the correlation between variables, to control for potential collinearity.

```{r ames_corplot, fig.align='center', out.width='80%'}
ames_train %>%
    select_if(is.numeric) %>%
    cor(use = 'complete.obs') %>%
    GGally::ggcorr(label_size = 2, legend.position = 'bottom')
```

### Nominal Variables

The data set has 46 columns which are nominal (all encoded as factors).

```{r}
ames_train %>%
    select_if(is.factor) %>%
    glimpse()
```

#### Garage Variables

There are 4 columns on *garage* which are nominal:

* Garage type (`Garage_Type`): garage location
* Garage finish (`Garage_Finish`): interior finish of the garage
* Garage quality (`Garage_Qual`): garage quality
* Garage condition (`Garage_Cond`): garage condition

```{r garage_bar_plot, fig.align='center', out.width='80%'}
p1 <- nom_bar_plot(ames_train, 'Garage_Type', 'garage type')
p2 <- nom_bar_plot(ames_train, 'Garage_Finish', 'garage finish')
p3 <- nom_bar_plot(ames_train, 'Garage_Qual', 'garage quality')
p4 <- nom_bar_plot(ames_train, 'Garage_Cond', 'garage condition')


(p1 + p2) / (p3 + p4)
```

From the plot above, we see that there are 2 variables which are heavily concentrated in just one factor (`Garage_Qual` and `Garage_Cond`).

```{r, collapse=TRUE}
# garage quality frequency table
ames_train %>% janitor::tabyl(Garage_Qual)

# garage condition frequency table
ames_train %>% janitor::tabyl(Garage_Cond)
```

Both variables have the '*typical*' class which weights ~90%. There are two solutions to this problem: cancel the columns or lump all the factors below a certain threshold.

```{r, collapse=TRUE}
# garage quality
ames_train %>%
    mutate(Garage_Qual = forcats::fct_lump_prop(Garage_Qual, prop = 0.05)) %>%
    janitor::tabyl(Garage_Qual)

# garage condition
ames_train %>%
    mutate(Garage_Cond = forcats::fct_lump_prop(Garage_Cond, prop = 0.05)) %>%
    janitor::tabyl(Garage_Cond)
```

A less problematic variable is `Garage_Type`, where the factors are more spread apart.

```{r, collapse=TRUE}
# garage type frequency table
ames_train %>% janitor::tabyl(Garage_Type)
```

In this case a factor lump with a 5% threshold should be fine

```{r, collapse=TRUE}
ames_train %>%
    mutate(Garage_Type = forcats::fct_lump_prop(Garage_Type, prop = 0.05)) %>%
    janitor::tabyl(Garage_Type)
```


#### Basement Variables

```{r, fig.align='center', out.width='80%', fig.asp=0.85}
p1 <- nom_bar_plot(ames_train, 'Bsmt_Qual', 'quality')
p2 <- nom_bar_plot(ames_train, 'Bsmt_Cond', 'condition')
p3 <- nom_bar_plot(ames_train, 'Bsmt_Exposure', 'exposure')
p4 <- nom_bar_plot(ames_train, 'BsmtFin_Type_1', 'type 1')
p5 <- nom_bar_plot(ames_train, 'BsmtFin_Type_2', 'type 2')

wrap_plots(p1, p2, p3, p4, p5, ncol = 3)
```

Basemend condition and basement type 2 seems to be variables with too much values in just one factor.

```{r, collapse=TRUE}
# bsm condition frequency table
ames_train %>% janitor::tabyl(Bsmt_Cond)

# bsm condition frequency with factor lump
ames_train %>%
    mutate(Bsmt_Cond = forcats::fct_lump_prop(Bsmt_Cond, prop = 0.05)) %>%
    janitor::tabyl(Bsmt_Cond)
```

```{r, collapse=TRUE}
# bsm type2 frequency table
ames_train %>% janitor::tabyl(BsmtFin_Type_2)

# bsm type2 frequency with factor lump
ames_train %>%
    mutate(BsmtFin_Type_2 = forcats::fct_lump_prop(BsmtFin_Type_2, 
                                                   prop = 0.05)) %>%
    janitor::tabyl(BsmtFin_Type_2)
```


## Linear Regression
***

Basic fit of all predictors vs log-sale prices. This is going to be the benchmark. Using `recipe::step_log` to log-transform the target variable in order to reduce skeweness and `recipe::step_dummy` to convert nominal values to dummy variables.


```{r lin_reg_model, message=FALSE, fig.align='center', out.width='80%'}
# set the model
lm_spec <- linear_reg() %>%
    set_mode('regression') %>%
    set_engine('lm')

# set the recipe
lin_reg_rec <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
    step_log(Sale_Price, base = 10, skip = TRUE) %>%
    step_other(Garage_Qual, Garage_Cond, Garage_Type, threshold = 0.05) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

# create model fitting workflow
lin_reg_wf <- workflow() %>%
    add_recipe(lin_reg_rec) %>%
    add_model(lm_spec)
  
# fit the model on the training data
lin_reg_fit <- fit(lin_reg_wf, data = ames_train)

# get RMSE on test data as base for models comparison
augment(lin_reg_fit, new_data = ames_test) %>%
    select(Sale_Price, .pred) %>%
    mutate(.pred = 10^.pred) %>%
    ggplot(aes(x = Sale_Price, y = .pred)) +
    geom_point(alpha = .4) +
    geom_abline(lty = 2, color = 'gray50', size = 1.2) +
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::number_format()) +
    theme_minimal()
```

## Ridge Regression
***

*Ridge regression* is very similar to OLS, except that the coefficients are estimated by minimizing a different quantity. The *ridge regression* coefficients are the values that minimize

$$\sum^n_{i=1}\bigg(y_i - \beta_0 - \sum^p_{j=1} \beta_j x_{ij} \bigg) + \lambda \sum^p_{j=1} \beta^2_j = RSS + \lambda \sum^p_{j=1} \beta^2_j$$

where $\lambda$ is a tuning parameter. Like OLS, ridge regression seeks coefficients estimates that minimize the RSS, however the *shrinkage penalty*, also called $L_2$, $\lambda \sum^p_{j=1} \beta^2_j$ is small when $\beta_1,...,\beta_p$ are close to zero, effectively shrinking the estimates of $\beta_j$ towards zero. The tuning parameter $\lambda$ serves to control the relative impact of these two terms; when $\lambda = 0$ the penalty term has no effect and the ridge regression will produce the least squared estimates. As $\lambda \rightarrow \infty$ the impact of the penalty term grows and the coefficient estimates will approach zero.

We will use the `glmnet` package to perform the ridge regression. Because parsnip doesn't have a dedicated function, we're going to use the `linear_reg` specification with `mixture = 0`; the *mixture* parameter specifies if we want to use a ridge regularization (value 0), a lasso regularization (value 1) or an elastic net regularization (between 0 and 1).  
When using the `glmnet` engine, we also need to set a *penalty* to fit the model.

```{r ridge_lamba_plot, eval=FALSE, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
ridge_spec <- linear_reg(mixture = 0, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# fit on train data
ridge_fit <- ridge_spec %>%
    fit(Sale_Price ~ ., ames_train)

# plot coefficients regularization
ridge_fit %>%
    extract_fit_engine() %>%
    plot(xvar = 'lambda')
```

The *penalty* value is a tunable one; we can fit multiple models with different values and check which one is the best. To tune it we need a workflow object containing the model and preprocessor, an rset object containing the resamples to be fitted and a tibble containing the parameter values to be evaluated.

```{r, message=FALSE}
# set model spec
ridge_spec <- linear_reg(mixture = 0, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# set recipe
ridge_rec <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
    step_log(Sale_Price, base = 10, skip = TRUE) %>%
    step_other(Garage_Qual, Garage_Cond, Garage_Type, threshold = 0.05) %>%
    step_novel(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_predictors())

# set workflow
ridge_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(ridge_spec)

# create the 'penalty' grid search for tuning
penalty_grid <- grid_regular(penalty(range = c(-5, 5)),
                             levels = 50)

# tune ridge penalty hyperparameter
doParallel::registerDoParallel()
set.seed(111)

ridge_tune <- tune_grid(ridge_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)
```

```{r, fig.align='center'}
autoplot(ridge_tune)
```

```{r ridge_show_metrics, collapse=TRUE}
ridge_tune %>%
    show_best(metric = 'rsq')

ridge_tune %>%
    show_best(metric = 'rmse')
```

```{r ridge_vip_plot, fig.align='center', fig.asp=0.85}
ridge_rmse <- ridge_tune %>% select_best('rmse')

final_ridge <- finalize_workflow(ridge_wf, ridge_rmse)

final_ridge %>%
    last_fit(ames_split) %>%
    extract_fit_engine() %>%
    vip::vi(lambda = ridge_rmse$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable   = forcats::fct_reorder(Variable, Importance)) %>%
    top_n(n = 25, wt = Importance) %>%
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(title = 'Top 25 variables by Importance')
```

```{r}
last_fit(final_ridge, ames_split) %>% collect_metrics()
```

```{r, fig.align='center', out.width='80%'}
final_ridge %>%
    last_fit(ames_split) %>%
    collect_predictions() %>%
    mutate(.pred = 10^.pred) %>%
    ggplot(aes(x = Sale_Price, y = .pred)) +
    geom_point(alpha = .3) +
    geom_abline(lty = 2, color = 'gray50', size = 1.2) +
    theme_minimal() +
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::number_format())
```


## Lasso Regression
***

Use mixture = 1 in glmnet.

```{r lasso_lamba_plot, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
lasso_spec <- linear_reg(mixture = 1, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# fit on train data
lasso_fit <- lasso_spec %>%
    fit(Sale_Price ~ ., ames_train)

# plot coefficients regularization
lasso_fit %>%
    extract_fit_engine() %>%
    plot(xvar = 'lambda')
```


We can re-use the ridge recipe in the lasso workflow

```{r}
lasso_spec <- linear_reg(mixture = 1, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

lasso_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(lasso_spec)

penalty_grid <- grid_regular(penalty(range = c(-5, 5)),
                             levels = 50)

lasso_tune <- tune_grid(lasso_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)

```

```{r, fig.align='center'}
autoplot(lasso_tune)
```

```{r, fig.align='center', out.width='80%', fig.asp=0.85}
lasso_penalty <- lasso_tune %>% select_best(metric = 'rmse')

final_lasso <- finalize_workflow(lasso_wf, lasso_penalty)

final_lasso %>%
    fit(data = ames_train) %>%
    extract_fit_parsnip() %>%
    vip::vi(lambda = lasso_penalty$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable   = forcats::fct_reorder(Variable, Importance)) %>%
    top_n(n = 25, wt = Importance) %>%
    ggplot(aes(x = Importance, y = Variable, color = Sign)) +
    geom_point() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(title = 'Top 25 Influential Variables', y = '') +
    theme_minimal()
```

```{r, fig.align='center', out.width='80%'}
final_lasso %>%
    last_fit(ames_split) %>%
    collect_predictions() %>%
    mutate(.pred = 10^.pred) %>%
    ggplot(aes(x = Sale_Price, y = .pred)) +
    geom_point(alpha = .3) +
    geom_abline(lty = 2, color = 'gray50', size = 1.2) +
    theme_minimal() +
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::number_format())
```

## Elastic Net
***

*Elastic Net* is a generalization of the ridge and lasso models, which combines the two penalties:

**BIG FORMULA**

The advantage of *elastic net* model is that it enables effective regularization via the ridge penalty with the feature selection characteristic of the lasso penalty.

In risge and lasso models $\lambda$ is the primary tuning parameter, but with elastic nets, we want to tune the $\lambda$ and the `mixture` parameters.

```{r}
# set elatic net model
elnet_spec <- linear_reg(mixture = tune(), penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# workflow (use ridge recipe)
elnet_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(elnet_spec)

# hyper-parameters grid
elnet_grid <- grid_regular(mixture(range = c(0, 1)),
                           penalty(range = c(-5, 5)),
                           levels = c(10, 40))
# activate parallel computing
#doParallel::registerDoParallel()

# set seed for reproducibility
set.seed(2020)

# tune elastic net model
elnet_tune <- tune_grid(elnet_wf,
                        resamples = ames_folds,
                        grid      = elnet_grid)

# plot RMSE / R-squared
autoplot(elnet_tune)
```

```{r, collapse=TRUE}
# best 5 models by R-sq
elnet_tune %>% show_best('rsq')

# best 5 models by RMSE
elnet_tune %>% show_best('rmse')
```

