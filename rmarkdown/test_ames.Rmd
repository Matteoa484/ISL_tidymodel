---
title: "test Ames"
date: "8/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
***

This is a test on the Ames data set, using different machine learning models.

## Libraries
***

```{r libraries, message=FALSE, warning=FALSE}
library(AmesHousing)
library(ggplot2)
library(tidymodels)
library(dplyr)
library(patchwork)
library(kableExtra)
```

## Functions
***

```{r functions}

tidy_tbl <- function(fit_obj) {
    
    data_tbl <- fit_obj %>%
        broom::tidy()
  
    

    table <- data_tbl %>%
        kbl() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                      full_width = FALSE,
                      position = 'left') %>%
        column_spec(5, color = 'white',
                    background = spec_color(data_tbl$p.value,
                                            begin = .2, end = .7,
                                            option = 'D',
                                            direction = -1))
  
    return(table)
  
}


## Function to plot the frequency of nominal variables
nom_bar_plot <- function(dataset, col, title) {
  
  if(is.character(col)) {
      col <- sym(col)
  } else {
      col <- enquo(col)
  }

  ggplot(dataset, aes(y = forcats::fct_infreq(!!col))) +
      geom_bar(fill = 'steelblue', alpha = .5) +
      labs(title = glue::glue('{col}'), x = '', y = '') +
      theme_minimal()

}

## function for scatter plot
num_xy_plot <- function(dataset, var, target) {
  
  ggplot(dataset, aes(x = {{var}}, y = {{target}})) +
      geom_point(fill = 'steelblue', alpha = .8) +
      theme_minimal()
  
}

```

## Data
***

All residential home sales in Ames, Iowa between 2006 and 2010. The data set contains many explanatory variables on the quality and quantity of physical attributes of residential homes in Iowa sold between 2006 and 2010. Most of the variables describe information a typical home buyer would like to know about a property (square footage, number of bedrooms and bathrooms, size of lot, etc.). A detailed discussion of variables can be found in the original paper ^[De Cock D. 2011. Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education; 19(3)]

The data is loaded into the package `AmesHousing`; it allows the user to work on the raw data (`ames_raw`) or on a pre-processed data set (`make_ames`), which is the one used in this example.

```{r make_data}
ames <- make_ames()
```

The raw data set is comprised of 82 fields (*variables*) recorded for 2.930 properties in Ames IA (*observations*).

For the processed version a summary of the differences between these data sets and `ames_raw` is:

* All factors are unordered.  
* PID and Order are removed.  
* Spaces and special characters in column names where changed to snake case. To be consistent, SalePrice was changed to Sale_Price.  
* Many factor levels were changed to be more understandable (e.g. Split_or_Multilevel instead of 080)  
* Many missing values were reset. For example, if the variable Bsmt_Qual was  missing, this implies that there is no basement on the property. Instead of a  missing value, the value of Bsmt_Qual was changed to No_Basement. Similarly,  numeric data pertaining to basements were set to zero where appropriate such as  variables Bsmt_Full_Bath and Total_Bsmt_SF.  
* Garage_Yr_Blt contained many missing data and was removed.  
* Approximate longitude and latitude are included for the properties. Also, note  that there are 6 properties with identical geotags. These are units within the same building. For some properties, updated versions of the PID identifiers were found  and are replaced with new values.

## Data Partition
***

At the start of a project, there's usually a finite pool of data available. The idea of *data spending* is an important first consideration when modeling, especially as it relates to empirical validation.

If the initial pool of data available is not huge, there will be some overlap of how and when the data is spent or allocated.

The primary approach is to *split* the existing data into two distinct sets:

* *Training Set*: usually the majority of the data, it is used to develop and optimize the model; these data are sandbox for model building, where different models can be fit, feature engineering strategies are investigated and so on.
* *Test Set*: the remaining portion of observations; it is held in reserve until one or two models are chosen as the most likely to succeed. The *test set* is then used as the final arbiter to determine the efficacy of the models.

Tidymodels allows to make random sampling splits with `rsample::initial_split`, which takes as arguments the data set to be splitted and the training proportion. Once the `rsplit` object is created, the *training* and *testing* sets can be created with `rsample::training()` and `rsample::testing` functions.  
The outputs are data frame with the same columns as the original data, but only the appropriate rows for each set.

Simple random sampling is appropriate in many cases, but when there is a dramatic *class imbalance* (one class / value occurs much less frequently than another) using a simple random sample may allocate those infrequently samples disproportionately into the training or testing set. To avoid this, *stratified sampling* can be used; the training/testing split is conducted separately within each class and then these sub-samples are combined into the overall training and testing set.  
For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times. This is an effective method for keeping the distribution of the outcome similar between the training and test set. In `rsample` the stratified variable can be selected using the option `strata = variable_name`.

```{r data_partition, message=FALSE}
# set seed for reproducibility
set.seed(123)

# main split (stratified by Sale_Price)
ames_split <- initial_split(ames, strata = Sale_Price, prop = .8)

# train and test data sets
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)

# set seed for reproducibility
set.seed(456)
# cross-validation data set (n = 10) from train 
ames_folds <- vfold_cv(ames_train, v = 10, strata = Sale_Price)
```


## Target Variable
***

The **target variable** is `Sale_Price`. Below its distribution.

```{r, message=FALSE, warning=FALSE, fig.align='center', out.width='80%'}
px_dist <- ames_train %>%
    select(Sale_Price) %>%
    ggplot(aes(x = Sale_Price)) +
    geom_histogram(bins = 60, alpha = .75, fill = 'steelblue') +
    geom_vline(xintercept = median(ames$Sale_Price),
               color = 'red', lty = 'dashed') +
    geom_vline(xintercept = mean(ames$Sale_Price),
               color = 'darkgreen', lty = 'solid') +
    scale_x_continuous(labels = scales::label_number(),
                       breaks = scales::breaks_pretty(n = 7),
                       expand = c(0.01, 0.01)) +
    labs(title = 'Sale Prices') +
    theme_minimal()

px_dist
```

The histogram shows that the most common selling level to be around \$160'000 (*median*) and the average selling price to be \$180'000 (*mean*). The plot shows also a right skewed distribution which suggest there are concern with an assumption of normality. To solve this problem it could be better to log-transform the variable.  

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.align='center'}
px_log_dist <- px_dist +
    scale_x_log10(labels = scales::label_number(),
                  expand = c(0.01, 0.01)) +
    labs(title = 'Log-Sale Prices')

px_dist | px_log_dist
```

The log-transformation helps to meet normality assumption, highlighting a couple of interesting information. The log-transformed distribution appears to be slightly multimodal and it helps us to see a couple of outliers, with one observation where the `Sale_Price` is close to zero.

Next check the correlation between variables, to control for potential collinearity.

```{r ames_corplot, fig.align='center', out.width='80%'}
ames_train %>%
    select_if(is.numeric) %>%
    cor(use = 'complete.obs') %>%
    GGally::ggcorr(label_size = 2, legend.position = 'bottom')
```

## Linear Regression
***

Compare two model, one raw and one pre-processed

Basic fit of all predictors vs sale prices. This is going to be the benchmark. Using `recipe::step_dummy` to convert nominal values to dummy variables.

See how data preprocessing can increase a simple model performance.

First declare the model specification, which will be used with all the different models.

```{r lm_spec}
# Model Spec
lr_spec <- linear_reg() %>%
    set_mode('regression') %>%
    set_engine('lm')
```

Then start with a basic recipe, converting to dummy and check the most important variables (highest t-stat with low p-value)

```{r lm_1, message=FALSE, warning=FALSE, collapse=TRUE}
## Recipe
lr_rec <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
    step_dummy(all_nominal_predictors())

## Workflow
lr_wf <- workflow() %>%
    add_recipe(lr_rec) %>%
    add_model(lr_spec)

## Fit
lr_fit <- lr_wf %>% fit(data = ames_train)
  
## RMSE
lr_fit %>%
    augment(new_data = ames_train) %>%
    rmse(truth = Sale_Price, estimate = .pred)
```

Start by checking the highest t-stats

```{r}
lr_fit %>%
    tidy() %>%
    filter(p.value <= 5e-04) %>%
    top_n(n = 20L, wt = abs(statistic)) %>%
    arrange(desc(abs(statistic)))
```

It seems that some of the categorical variables have the highest t-stat. In particular `Roof_Matl`, `Misc_Feature`, `Condition_2`, `Neighborhood`; let's check the info in this col first. The `Neighborhood` has so many levels that it is better to create a separate plot.

```{r}
c('Roof_Matl', 'Misc_Feature', 'Condition_2') %>%
    purrr::map(nom_bar_plot, dataset = ames_train, title = .x) %>%
    wrap_plots(ncol = 2)
```

The plots above show a problem with all three variables: one class dominate the others.

```{r, collapse=TRUE}
# roof material
ames_train %>%
    janitor::tabyl(Roof_Matl)
# condition 2
ames_train %>%
    janitor::tabyl(Condition_2)
```

Having a column with a values equal to 99% of the observation doesn't make a lot of sense. The next recipe will delete `Roof_Matl` and `Condition_2` from the dataset and will collapse the factors levels of `Misc_Features` with a 3% threshold.

Let's check `Neighborhood`

```{r}
nom_bar_plot(ames_train, 'Neighborhood')
```

Here there are a couple of levels which are rare. Here as well it makes sense to lump the factors with a 3% limit.

Moving to numerical predictors, we see that `Second_Flr_SF`, `First_Flr_SF`, `Bsmt_Unf_SF`, `Lot_Area` and `Year_Built` have a high t-stat.  
Below the relative distributions, all more or less right-skewed.

```{r}
ames_train %>%
    select(Sale_Price, Second_Flr_SF, First_Flr_SF, 
           Bsmt_Unf_SF, Lot_Area, Year_Built) %>%
    tidyr::pivot_longer(-Sale_Price, names_to = 'var', values_to = 'value') %>%
    ggplot(aes(x = value)) +
    geom_histogram(bins = 60) +
    facet_wrap(~var, scales = 'free')
```

Let's try to check the relationship between the variable and the target.

```{r}
p1 <- ggplot(ames_train, aes(x = Year_Built, y = Sale_Price)) +
      geom_point(color = 'steelblue', alpha = .5) +
      theme_minimal()

p2 <- ggplot(ames_train, aes(x = Bsmt_Unf_SF, y = Sale_Price)) +
      geom_point(color = 'steelblue', alpha = .5) +
      theme_minimal()

<<<<<<< HEAD
p3 <- ggplot(ames_train, aes(x = First_Flr_SF, y = Sale_Price)) +
      geom_point(color = 'steelblue', alpha = .5) +
      theme_minimal()
=======
Basic fit of all predictors vs sale prices. This is going to be the benchmark. Using `recipe::step_dummy` to convert nominal values to dummy variables, `step_other` to pool infrequently nominal values into an 'other' category and `step_novel` to assign a previously unseen factor level to a new value.
>>>>>>> 9be2c359f62fd6b8787b76c30525d284c44b4b72

p4 <- ggplot(ames_train, aes(x = Lot_Area, y = Sale_Price)) +
      geom_point(color = 'steelblue', alpha = .5) +
      theme_minimal()

(p1 + p2) / (p3 + p4)
```

<<<<<<< HEAD
From the plot above the best insight is that the relationship between `Year_Built` and `Sale_Price` is not linear; it seems that a polynomial should increase the precision.

```{r lr_rec1}
=======
>>>>>>> 9be2c359f62fd6b8787b76c30525d284c44b4b72
## Recipe
lr_rec1 <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
<<<<<<< HEAD
    # remove worthless vars
    step_rm(Roof_Matl, Condition_2) %>%
    step_other(Misc_Feature, Neighborhood, threshold = 0.03) %>%
    step_poly(Year_Built, degree = 2) %>%
    step_dummy(all_nominal_predictors())
=======
    step_dummy(all_nominal_predictors()) %>%
    step_other(all_nominal_predictors(), threshold = 0.06) %>% # pool categories < 3%
    step_novel(all_nominal_predictors())
>>>>>>> 9be2c359f62fd6b8787b76c30525d284c44b4b72

## Workflow
lr_wf <- workflow() %>%
    add_recipe(lr_rec1) %>%
    add_model(lr_spec)

## Fit
lr_fit1 <- lr_wf %>% fit(data = ames_train)
  
## RMSE
lr_fit1 %>%
    augment(new_data = ames_train) %>%
    rmse(truth = Sale_Price, estimate = .pred)
```

<<<<<<< HEAD


```{r}
lr_fit %>%
    extract_fit_engine() %>%
    vip::vip(num_features = 20L)
```


## Ridge Regression
=======
## Penalized Regression
>>>>>>> 9be2c359f62fd6b8787b76c30525d284c44b4b72
***

In general the *penalized regressions*, Ridge, Lasso and ElasticNet, are a way to augment the reduce bias in order to reduce the variance. They do this by adding a penalty to the OLS, this penalty basically reduce the variables' $\beta$.

### Ridge Regression

*Ridge regression* is very similar to OLS, except that the coefficients are estimated by minimizing a different quantity. The *ridge regression* coefficients are the values that minimize

$$\sum^n_{i=1}\bigg(y_i - \beta_0 - \sum^p_{j=1} \beta_j x_{ij} \bigg) + \lambda \sum^p_{j=1} \beta^2_j = RSS + \lambda \sum^p_{j=1} \beta^2_j$$

where $\lambda$ is a tuning parameter. Like OLS, ridge regression seeks coefficients estimates that minimize the RSS, however the *shrinkage penalty*, also called $L_2$, $\lambda \sum^p_{j=1} \beta^2_j$ is small when $\beta_1,...,\beta_p$ are close to zero, effectively shrinking the estimates of $\beta_j$ towards zero. The tuning parameter $\lambda$ serves to control the relative impact of these two terms; when $\lambda = 0$ the penalty term has no effect and the ridge regression will produce the least squared estimates. As $\lambda \rightarrow \infty$ the impact of the penalty term grows and the coefficients are reduced to a point where they will approach zero.

```{r ridge_lamba_plot, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
linear_reg(mixture = 0, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet') %>%
    # fit on train data
    fit(Sale_Price ~ ., ames_train) %>%
    # plot coefficients regularization
    .$fit %>%
    plot(xvar = 'lambda')
```

The engine to use for *ridge regression* is `glmnet`. In the model specification we need to select `linear_reg` with `mixture = 0`; the *mixture* parameter specifies if we want to use a ridge regularization (value 0), a lasso regularization (value 1) or an elastic net regularization (between 0 and 1).  
The `penalty` value is the equivalent of $\lambda$ and it is a tunable parameter; to tune the model we need a `workflow` object containing:

* a `parsnip` model specification
* a `recipe` preprocessor
* an `rset` object containing the resamples to be fitted
* a `tibble` with the parameter values to be evaluated.

```{r ridge_model, message=FALSE}
# set model spec
ridge_spec <- linear_reg(mixture = 0, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# set recipe
ridge_rec <-
    lr_rec %>%
    step_zv(all_predictors()) %>%
    # ridge needs normalized variables
    step_normalize(all_predictors())

# set workflow
ridge_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(ridge_spec)

# create the 'penalty' grid search for tuning
penalty_grid <- grid_regular(penalty(range = c(-10, 10)), levels = 50)

# tune ridge penalty hyperparameter
n_core <- parallel::detectCores() -1
cluster <- parallel::makeCluster(n_core)

doParallel::registerDoParallel(cl = cluster)
set.seed(111)

ridge_tune <- tune_grid(ridge_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)
```

```{r ridge_tune_plot, fig.align='center'}
ridge_best <- select_best(ridge_tune, metric = 'rmse')
ridge_1sd <- select_by_one_std_err(ridge_tune, metric = 'rmse', penalty)

ridge_tune %>%
    collect_metrics() %>%
    filter(.metric == 'rmse') %>%
    ggplot(aes(x = penalty, y = mean)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = ridge_best$penalty, lty = 2, color = 'darkgray') +
    geom_vline(xintercept = ridge_1sd$penalty, lty = 2, color = 'red') +
    scale_x_log10() +
    scale_y_continuous(labels = scales::number_format(), expand = c(0.01, 0.01)) +
    labs(title = 'Ridge Tuning', x = 'log-penalty', y = 'RMSE') +
    theme_minimal()
```

```{r ridge_show_metrics, collapse=TRUE}
ridge_tune %>%
    show_best(metric = 'rsq')

ridge_tune %>%
    show_best(metric = 'rmse')
```

```{r ridge_vip, fig.align='center', fig.asp=0.85}
ridge_rmse <- ridge_tune %>% select_best('rmse')

final_ridge <- finalize_workflow(ridge_wf, ridge_rmse)

final_ridge %>%
    fit(ames_train) %>%
    extract_fit_engine() %>%
    vip::vi(lambda = ridge_rmse$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable = forcats::fct_reorder(Variable, Importance)) %>%
    slice_max(Importance, n = 20) %>%
    ggplot(aes(x = Importance, y = Variable, color = Sign)) +
    geom_point() +
    theme_minimal() +
    theme(legend.position = 'none')
```

```{r}
last_fit(final_ridge, ames_split) %>% collect_metrics()
```

```{r, fig.align='center', out.width='80%'}
final_ridge %>%
    last_fit(ames_split) %>%
    collect_predictions() %>%
    ggplot(aes(x = Sale_Price, y = .pred)) +
    geom_point(alpha = .3) +
    geom_abline(lty = 2, color = 'gray50', size = 1.2) +
    theme_minimal() +
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::number_format())
```


### Lasso Regression
***

The *least absolute shrinkage and selection operator* (lasso) model is an alternative to ridge regression that has a small modification to the penalty in the objective function. Rather than the $L_2$ penalty it uses the $L_1$ penalty $\lambda\sum^p_{j=1}|\beta_j|$ in the objective function.

The lasso coefficients are the ones that minimize the function:

$$\sum^n_{i=1}\bigg(y_i - \beta_0 - \sum^p_{j=1} \beta_j x_{ij} \bigg) + \lambda \sum^p_{j=1} |\beta_j| = RSS + \lambda \sum^p_{j=1} |\beta_j|$$

Whereas the ridge regression approach pushes variables to approximately but not equal to zero, the lasso penalty will actually push coefficients to zero. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection.

Use mixture = 1 in glmnet.

```{r lasso_lamba_plot, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
linear_reg(mixture = 1, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet') %>%
    # fit on train data
    fit(Sale_Price ~ ., ames_train) %>%
    # plot coefficients regularization
    .$fit %>%
    plot(xvar = 'lambda')
```


We can re-use the ridge recipe in the lasso workflow

```{r lasso_model}
lasso_spec <- linear_reg(mixture = 1, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

lasso_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(lasso_spec)

penalty_grid <- grid_regular(penalty(range = c(-10, 10)),
                             levels = 50)

lasso_tune <- tune_grid(lasso_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)

```

```{r, fig.align='center'}
lasso_best <- select_best(lasso_tune, metric = 'rmse')
lasso_1sd <- select_by_one_std_err(lasso_tune, metric = 'rmse', penalty)

lasso_tune %>%
    collect_metrics() %>%
    filter(.metric == 'rmse') %>%
    ggplot(aes(x = penalty, y = mean)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = lasso_best$penalty, lty = 2, color = 'darkgray') +
    geom_vline(xintercept = lasso_1sd$penalty, lty = 2, color = 'red') +
    scale_x_log10(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::number_format(), expand = c(0.01, 0.01)) +
    labs(title = 'Lasso Tuning', x = 'log-penalty', y = 'RMSE') +
    theme_minimal()
```

```{r, fig.align='center', out.width='80%', fig.asp=0.85}
lasso_penalty <- lasso_tune %>% select_best(metric = 'rmse')

final_lasso <- finalize_workflow(lasso_wf, lasso_penalty)

final_lasso %>%
    fit(ames_train) %>%
    extract_fit_engine() %>%
    vip::vi(lambda = lasso_penalty$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable   = forcats::fct_reorder(Variable, Importance)) %>%
    slice_max(Importance, n = 20) %>%
    ggplot(aes(x = Importance, y = Variable, color = Sign)) +
    geom_point() +
    theme_minimal() +
    theme(legend.position = 'none')
```

```{r, fig.align='center', out.width='80%'}
final_lasso %>%
    last_fit(ames_split) %>%
    collect_predictions() %>%
    ggplot(aes(x = Sale_Price, y = .pred)) +
    geom_point(alpha = .3) +
    geom_abline(lty = 2, color = 'gray50', size = 1.2) +
    theme_minimal() +
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::number_format())
```

### Elastic Net
***

*Elastic Net* is a generalization of the ridge and lasso models, which combines the two penalties:

**BIG FORMULA**

The advantage of *elastic net* model is that it enables effective regularization via the ridge penalty with the feature selection characteristic of the lasso penalty.

In risge and lasso models $\lambda$ is the primary tuning parameter, but with elastic nets, we want to tune the $\lambda$ and the `mixture` parameters.

```{r}
# set elatic net model
elnet_spec <- linear_reg(mixture = tune(), penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# workflow (use ridge recipe)
elnet_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(elnet_spec)

# hyper-parameters grid
elnet_grid <- grid_regular(mixture(range = c(0, 1)),
                           penalty(range = c(-5, 5)),
                           levels = c(10, 40))
# activate parallel computing
#doParallel::registerDoParallel()

# set seed for reproducibility
set.seed(2020)

# tune elastic net model
elnet_tune <- tune_grid(elnet_wf,
                        resamples = ames_folds,
                        grid      = elnet_grid)

# plot RMSE / R-squared
autoplot(elnet_tune)
```

```{r, collapse=TRUE}
# best 5 models by R-sq
elnet_tune %>% show_best('rsq')

# best 5 models by RMSE
elnet_tune %>% show_best('rmse')
```

## Tree-based methods

Fit regression tree

```{r}
tree_spec <- decision_tree(cost_complexity = tune()) %>%
    set_engine('rpart') %>%
    set_mode('regression')

tree_wf <- workflow() %>%
    add_model(tree_spec) %>%
    add_formula(Sale_Price ~ .)

tree_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)

tree_tune <- tune_grid(tree_wf,
                       resamples = ames_folds,
                       grid      = tree_grid)
```

```{r}
autoplot(tree_tune)
```

```{r}
best_complexity <- select_best(tree_tune, metric = "rmse")

tree_final <- finalize_workflow(tree_wf, best_complexity)

tree_final_fit <- fit(tree_final, data = ames_train)
tree_final_fit
```

```{r}
tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(roundint = FALSE)
```

## Bagging

A bagging model is the same as a random forest where mtry is equal to the number of predictors. We can specify the mtry to be .cols() which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and usable to many different data sets. .cols() is one of many descriptors in the parsnip package. We also set importance = TRUE in set_engine() to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the vip package later.

```{r}
bagging_spec <- rand_forest(mtry = .cols()) %>%
    set_engine('randomForest', importance = TRUE) %>%
    set_mode('regression')

bagging_fit <- fit(bagging_spec, Sale_Price ~ ., data = ames_train)
```

```{r}
bagging_fit %>%
    augment(new_data = ames_train) %>%
    rmse(truth = Sale_Price, estimate = .pred)
```

Next, let us take a look at the variable importance

```{r}
vip::vip(bagging_fit)
```

