---
title: "test Ames"
date: "8/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
***

This is a test on the Ames data set, using different machine learning models.

## Libraries
***

```{r libraries, message=FALSE, warning=FALSE}
library(AmesHousing)
library(ggplot2)
library(tidymodels)
library(dplyr)
library(patchwork)
library(kableExtra)
```

## Functions
***

```{r functions}

tidy_tbl <- function(fit_obj) {
    
    data_tbl <- fit_obj %>%
        broom::tidy()
  
    

    table <- data_tbl %>%
        kbl() %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                      full_width = FALSE,
                      position = 'left') %>%
        column_spec(5, color = 'white',
                    background = spec_color(data_tbl$p.value,
                                            begin = .2, end = .7,
                                            option = 'D',
                                            direction = -1))
  
    return(table)
  
}

```

## Data
***

All residential home sales in Ames, Iowa between 2006 and 2010. The data set contains many explanatory variables on the quality and quantity of physical attributes of residential homes in Iowa sold between 2006 and 2010. Most of the variables describe information a typical home buyer would like to know about a property (square footage, number of bedrooms and bathrooms, size of lot, etc.). A detailed discussion of variables can be found in the original paper referenced below.

*De Cock D. 2011. Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education; 19(3)*

The data is loaded into the package `AmesHousing`; it allows the user to work on the raw data (`ames_raw`) or on a pre-processed data set (`make_ames`), which is the one used in this example.

The raw data set is comprised of 82 fields (*variables*) recorded for 2.930 properties in Ames IA (*observations*).

For the processed version a summary of the differences between these data sets and `ames_raw` is:

* All factors are unordered.  
* PID and Order are removed.  
* Spaces and special characters in column names where changed to snake case. To be consistent, SalePrice was changed to Sale_Price.  
* Many factor levels were changed to be more understandable (e.g. Split_or_Multilevel instead of 080)  
* Many missing values were reset. For example, if the variable Bsmt_Qual was  missing, this implies that there is no basement on the property. Instead of a  missing value, the value of Bsmt_Qual was changed to No_Basement. Similarly,  numeric data pertaining to basements were set to zero where appropriate such as  variables Bsmt_Full_Bath and Total_Bsmt_SF.  
* Garage_Yr_Blt contained many missing data and was removed.  
* Approximate longitude and latitude are included for the properties. Also, note  that there are 6 properties with identical geotags. These are units within the same building. For some properties, updated versions of the PID identifiers were found  and are replaced with new values.

```{r}
ames <- make_ames()
```

## EDA
***

Some basic EDA on the data set.

```{r}
skimr::skim(ames)
```

```{r}
glimpse(ames)
```

```{r}
head(ames)
```

The **target variable** is `Sale_Price`. Below its distribution.

```{r, message=FALSE, warning=FALSE}
px_dist <- ames %>%
    select(Sale_Price) %>%
    ggplot(aes(x = Sale_Price)) +
    geom_histogram(bins = 60, alpha = .75, fill = 'steelblue') +
    geom_vline(xintercept = median(ames$Sale_Price),
               color = 'red', lty = 'dashed') +
    geom_vline(xintercept = mean(ames$Sale_Price),
               color = 'darkgreen', lty = 'solid') +
    scale_x_continuous(labels = scales::label_number(),
                       breaks = scales::breaks_pretty(n = 7),
                       expand = c(0.01, 0.01)) +
    labs(title = 'Sale Prices') +
    theme_minimal()

px_dist
```

The histogram shows that the most common selling level to be around \$160'000 (*median*) and the average selling price to be \$180'000 (*mean*). The plot shows also a right skewed distribution which suggest there are concern with an assumption of normality. To solve this problem it is better to log-transform the variable.  
Before changing the data set, it's better to visualize the same distribution after a log-transformation.

```{r, message=FALSE, warning=FALSE}
px_log_dist <- px_dist +
    scale_x_log10(labels = scales::label_number(),
                  expand = c(0.01, 0.01)) +
    labs(title = 'Log-Sale Prices')

px_dist | px_log_dist
```

The log-transformation helps to meet normality assumption and helps to see a couple of outliers, with one observation where the `Sale_Price` is close to zero.

A good way to check for outliers is the box-plot:

```{r}
ames %>%
    select(Sale_Price) %>%
    ggplot(aes(x = 'var', y = Sale_Price)) +
    geom_boxplot(outlier.alpha = .4) +
    geom_jitter(alpha = .1, width = .3) +
    scale_y_log10(labels = scales::label_number(),
                       breaks = quantile(ames$Sale_Price)) +
    labs(title = 'Sale Price Box-Plot') +
    theme_minimal()
    
```

Next check the correlation between variables, to control for potential collinearity.

```{r}
ames %>%
    select_if(is.numeric) %>%
    cor(use = 'complete.obs') %>%
    GGally::ggcorr(label_size = 2, legend.position = 'bottom')
```

## Data Partition
***

```{r data_partition, message=FALSE}
# set seed for reproductibility
set.seed(123)

# main split (stratified Sale_Price)
ames_split <- initial_split(ames, strata = Sale_Price, prop = .7)

# train and test data sets
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)

# cross-validation data set (n = 10) from train 
ames_folds <- vfold_cv(ames_train, v = 10)
```

## Linear Regression
***

Basic fit of all predictors vs log-sale prices. This is going to be the benchmark. Using `recipe::step_log` to log-transform the target variable in order to reduce skeweness and `recipe::step_dummy` to convert nominal values to dummy variables.


```{r lin_reg_model, message=FALSE}
# set the model
lm_spec <- linear_reg() %>%
    set_mode('regression') %>%
    set_engine('lm')

# set the recipe
lin_reg_rec <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
    step_log(Sale_Price, skip = TRUE) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors())

# create model fitting workflow
lin_reg_wf <- workflow() %>%
    add_recipe(lin_reg_rec) %>%
    add_model(lm_spec)
  
# fit the model on the training data
lin_reg_fit <- fit(lin_reg_wf, data = ames_train)

# get RMSE on test data as base for models comparison
augment(lin_reg_fit, new_data = ames_test) %>%
    rmse(truth = Sale_Price, estimate = .pred)
```

## Ridge Regression
***

*Ridge Regression* controls the coefficients by adding **FORMULA** to the objective function. This penalty parameter is also referred to al "$L_2$" as it signifies a second-order penalty being used on the coefficients.

**BIG FORMULA**

This penalty parameters can take a wide range of values, which is controlled by the tuning parameter $\lambda$. When $\lambda = 0$ there is no effect and our objective function equals the normal OLS regression objective function of simply minimizing the SSE.. However when $\lambda \rightarrow \infty$, the penalty becomes large and forces our coefficients to zero.

We will use the `glm` package to perform the ridge regression. Because parsnip doesn't have a dedicated function, we're going to use the `linear_reg` specification with `mixture = 0`; the *mixture* parameter specifies if we want to use a ridge regularization (value 0), a lasso regularization (value 1) or an elasticnet regularization (between 0 and 1).  
When using the `glm` engine, we also need to set a *penalty* to fit the model.

```{r ridge_lamba_plot, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
ridge_spec <- linear_reg(mixture = 0, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# fit on train data
ridge_fit <- ridge_spec %>%
    fit(Sale_Price ~ ., ames_train)

# plot coefficients regularization
ridge_fit %>%
    extract_fit_engine() %>%
    plot(xvar = 'lambda')
```

The *penalty* value is a tunable one; we can fit multiple models with different values and check which one is the best. To tune it we need a workflow object containing the model and preprocessor, an rset object containing the resamples to be fitted and a tibble containing the parameter values to be evaluated.

```{r, message=FALSE}
# set model spec
ridge_spec <- linear_reg(mixture = 0, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# set recipe (no target log-transformation)
ridge_rec <-
    recipe(Sale_Price ~ ., data = ames_train) %>%
    step_novel(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_predictors())

# set workflow
ridge_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(ridge_spec)

# create the 'penalty' grid search for tuning
penalty_grid <- grid_regular(penalty(range = c(-5, 5)),
                             levels = 50)

# tune ridge penalty hyperparameter
ridge_tune <- tune_grid(ridge_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)
```

```{r, fig.align='center'}
autoplot(ridge_tune)
```

```{r}
ridge_tune %>%
    collect_metrics()
```

```{r ridge_vip_plot, fig.align='center'}
ridge_rmse <- ridge_tune %>% select_best('rmse')

final_ridge <- finalize_workflow(ridge_wf, ridge_rmse)

final_ridge %>%
    fit(ames_train) %>%
    extract_fit_parsnip() %>%
    vip::vi(lambda = ridge_rmse$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable   = forcats::fct_reorder(Variable, Importance)) %>%
    top_n(n = 25, wt = Importance) %>%
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(title = 'Top 25 variables by Importance')
```

```{r}
last_fit(final_ridge, ames_split) %>% collect_metrics()
```

## Lasso Regression
***

Use mixture = 1 in glmnet.

```{r lasso_lamba_plot, cache=TRUE, fig.align='center'}
# set test model spec (penalty not-tuned)
lasso_spec <- linear_reg(mixture = 1, penalty = 0) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# fit on train data
lasso_fit <- lasso_spec %>%
    fit(Sale_Price ~ ., ames_train)

# plot coefficients regularization
lasso_fit %>%
    extract_fit_engine() %>%
    plot(xvar = 'lambda')
```


We can re-use the ridge recipe in the lasso workflow

```{r}
lasso_spec <- linear_reg(mixture = 1, penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

lasso_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(lasso_spec)

penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
                             levels = 50)

lasso_tune <- tune_grid(lasso_wf,
                        resamples = ames_folds,
                        grid      = penalty_grid)

```

```{r, fig.align='center'}
autoplot(lasso_tune)
```

```{r, fig.align='center', out.width='80%'}
lasso_penalty <- lasso_tune %>% select_best(metric = 'rmse')

final_lasso <- finalize_workflow(lasso_wf, lasso_penalty)

final_lasso %>%
    fit(data = ames_train) %>%
    extract_fit_parsnip() %>%
    vip::vi(lambda = lasso_penalty$penalty) %>%
    mutate(Importance = abs(Importance),
           Variable   = forcats::fct_reorder(Variable, Importance)) %>%
    top_n(n = 25, wt = Importance) %>%
    ggplot(aes(x = Importance, y = Variable, color = Sign)) +
    geom_point() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(title = 'Top 25 Influential Variables', y = '') +
    theme_minimal()
```

```{r}
last_fit(final_lasso, ames_split) %>% collect_metrics()
```

## Elastic Net
***

*Elastic Net* is a generalization of the ridge and lasso models, which combines the two penalties:

**BIG FORMULA**

The advantage of *elastic net* model is that it enables effective regularization via the ridge penalty with the feature selection characteristic of the lasso penalty.

In risge and lasso models $\lambda$ is the primary tuning parameter, but with elastic nets, we want to tune the $\lambda$ and the `mixture` parameters.

```{r}
# set elatic net model
elnet_spec <- linear_reg(mixture = tune(), penalty = tune()) %>%
    set_mode('regression') %>%
    set_engine('glmnet')

# workflow (use ridge recipe)
elnet_wf <- workflow() %>%
    add_recipe(ridge_rec) %>%
    add_model(elnet_spec)

# hyper-parameters grid
elnet_grid <- grid_regular(mixture(range = c(0, 1)),
                           penalty(range = c(-3, 3)),
                           levels = c(10, 40))
# activate parallel computing
doParallel::registerDoParallel()

# set seed for reproducibility
set.seed(2020)

# tune elastic net model
elnet_tune <- tune_grid(el_net_wf,
                        resamples = ames_folds,
                        grid      = elnet_grid)

# plot RMSE / R-squared
autoplot(elnet_tune)
```

```{r}
elnet_tune %>% select_best('rmse')
```

