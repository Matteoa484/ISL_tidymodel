---
title: "Linear Regression"
author: "Matteo Anro"
date: "6/18/2021"
output: 
  html_document:
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Exercises from Introduction to Statistical Learning for the Linear Regression chapter

## Libraries

```{r libraries, message=FALSE, warning=FALSE}
library(ISLR)
library(MASS)
library(dplyr)
library(tidymodels)
library(ggplot2)
```

## Simple Linear Regression

The `MASS` library contains the `Boston` data set, which records `medv` (median house value) for 506 neighborhoods around Boston. We will seek to predict `medv` using 13 predictors such as `rm` (average number of rooms per house), `age` (average age of house) and `lstat` (percent of households with low socioeconomic status).

```{r}
data(Boston)

head(Boston)
```

We will start fitting a simple linear regression model, with `medv` as the response and `lstat`as the predictor. As the plot below shows, there's some evidence of non-linearity in the relationship between the two variables:

```{r, fig.align='center', out.width='60%'}
ggplot(Boston) +
    geom_point(aes(x = lstat, y = medv), alpha = .5)
```


First we start by creating a `parsnip` specification for linear regression.

```{r}
lm_spec <- linear_reg() %>%
    set_mode('regression') %>%
    set_engine('lm')

lm_spec
```

The specification doesn't perform any calculations by itself, it is just a specification of what we want to do.

Once we have the specification, we can `fit`it by supplying a formula expression (in the form `y ~ x`) and the data.

```{r}
lm_fit <- lm_spec %>%
    fit(medv ~ lstat, data = Boston)

lm_fit
```

The result is a `parsnip` object, which contains the underlying fit as well as some parsnip-related information.

```{r}
names(lm_fit)
```

The fit results are stored in the data frame `fit`

```{r}
lm_fit$fit
```

For more detailed information we can pass the data frame to `summary`, which gives us p-values and standard errors for the coefficients, as well as the $R^2$ and $F$ statistics.

```{r}
summary(lm_fit$fit)
```

Another way to extract information in a tidy way is to use the helper functions from `broom`, passing directly the `parsnip` object.  
The `tidy` function returns the parameters estimates:

```{r}
tidy(lm_fit)
```

and `glance` can be used to extract the model statistics:

```{r}
glance(lm_fit)
```

The `predict` function can be used to get the predicted data.

```{r}
predict(lm_fit, new_data = Boston) %>%
    head()
```

We can also return other types of predictions by specifying the `type` argument. For example, setting `type = conf_int` returns a 95% confidence interval.

```{r}
predict(lm_fit, new_data = Boston, type = 'conf_int') %>%
    head()
```

In order to evaluate the performance of the model, we can compare the observed and the predicted values.

```{r, fig.align='center', out.width='60%'}
bind_cols(predict(lm_fit, new_data = Boston),
          Boston) %>%
    select(medv, .pred) %>%
    ggplot() +
    geom_point(aes(x = .pred, y = medv), alpha = .5)
```

The same results can be obtained with the `tune::augment` function.

```{r, fig.align='center', out.width='60%'}
augment(lm_fit, new_data = Boston) %>%
    select(medv, .pred) %>%
    ggplot() +
    geom_point(aes(x = .pred, y = medv), alpha = .5)
```

## Multiple Linear Regression

The multiple linear regression model can be fit in much the same way as the previous simple linear regression model. The only difference is how we specify the predictors, using the formula method, but adding several variables by separating them with `+`.

```{r}
lm_fit2 <- lm_spec %>% # same parsnip spec as before
    fit(medv ~ lstat + age, data = Boston)

lm_fit2
```

All the previous helper functions work the same. From extracting parameters:

```{r}
tidy(lm_fit2)
```

to predicting new values

```{r}
predict(lm_fit2, new_data = Boston) %>%
    head()
```

The `Boston` data set contains 13 variables, and so it would be cumbersome to type all of them in order to perform a regression using all the predictors. Instead we can use the notation `y ~ .`

```{r}
lm_fit3 <- lm_spec %>%
    fit(medv ~ ., data = Boston)

lm_fit3
```

```{r}
glance(lm_fit3)
```

If we want to perform a regression using all the variables except one, we can use the formula notation adding a `-` before the variable we don't want to use.

```{r, eval=FALSE}
lm_spec %>%
    fit(medv ~ . -age, data = Boston)
```

## Interaction terms

Including an interaction term can be done directly inside the formula notation using the syntax `lstat:black` to include an interaction term between `lstat` and `black` variables. The syntax `lstat*age` is a shorthand for `lstat + age + lstat:age`.
*Tidymodels* allow us to apply transformations as a pre-processing step, using `recipe`.
We use the `step_interact` to specify the interaction term. Next we create a workflow object to combine the model specification `lm_spec` with the pre-processing specification `rec_spec_interact` which can be fitted like a `parsnip` model.

```{r}
rec_spec_interact <- 
    recipe(medv ~ lstat + age, data = Boston) %>%
    step_interact(~ lstat:age)

lm_wf_interact <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(rec_spec_interact)

lm_wf_interact %>% fit(data = Boston)
```

## Non-linear transformation of the predictors

In base R the transformation can be done directly inside the formula notation, using the function `I()`.  
In *tidymodels* this transformations are part of the pre-processing and thus are added to a *recipe*. The `recipe` package has the `step_mutate` function which is pretty much the equivalent of `dplyr::mutate`.

In general you would want to keep as much of the pre-processing inside `recipe` such that the transformations will be applied consistently to new data.

```{r}
rec_spec_pow2 <- recipe(medv ~ lstat, data = Boston) %>%
    step_mutate(lstat2 = lstat ^ 2) # pow 2 to lstat variable

lm_wf_pow2 <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(rec_spec_pow2)

lm_wf_pow2 %>% fit(Boston)
```

Not all the transformations need to be done with `recipe::step_mutate` since it  has a bunch of them created already. For example `step_log` take the logarithm of the variables.

```{r}
rec_spec_log <- recipe(medv ~ lstat, data = Boston) %>%
    step_log(lstat)

lm_wf_log <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(rec_spec_log)

lm_wf_log %>% fit(Boston)
```

## Qualitative predictors

The following example is based on the `Carseat` data set from the package `ISLR`. We will predict `Sales` of child car seats in 400 locations based on a number of predictors. 

```{r}
head(Carseats)
```


One of those variables, `ShelveLoc`, is a qualitative predictor that indicates the quality of the shelving location. It can take three possible values:

* Bad
* Medium
* Good

If you pass such variable to `lm` it will read it and generate dummy variables using the following convention:

```{r}
Carseats %>%
    pull(ShelveLoc) %>%
    contrasts()
```

The problem is that not all the underlying engines know how to deal with qualitative variables, so we need a way to manage them before the model fitting. `recipe` has different function to deal with this kind of situation.  
The `step_dummy` will perform the transformation of turning one qualitative variable with x levels into x-1 dummy variables. In the example below we're also using the `all_nominal_predictors` function to execute the transformation on the character and factor variables.

```{r}
rec_spec <- recipe(Sales ~ ., data = Carseats) %>%
    step_dummy(all_nominal_predictors()) %>%
    step_interact(~ Income:Advertising + Price:Age)

lm_wf <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(rec_spec)

lm_wf %>% fit(Carseats)
```

# Classification

## Stock Market data

We need the `discrim` package for the discriminant analysis.

```{r, message=FALSE}
library(discrim)
```

We will be examining the `Smarket` data set, which contains a number of numeric variables plus a variable called `Direction` which has the two labels *Up* and *Down*.

```{r}
head(Smarket)
```

Let's take a look at the correlation of the data, removing `Direction` as it is not numeric.

```{r, fig.align='center', out.width='60%'}
Smarket %>%
    select(-Direction) %>%
    cor() %>%
    GGally::ggcorr(label = TRUE)
```

As shown by the corplot, the variables are more or less uncorrelated, except for `Year` and `Volume`.  
If we plot the two variables we can see that there is an upward trend in `Volume` with time.

```{r, fig.align='center', out.width='60%'}
ggplot(Smarket, aes(x = Year, y = Volume)) +
    geom_jitter(height = 0, alpha = .5)
```

##â™£ Logistic Regression

We will fit a *logistic regression* with the aid of `parsnip`.

```{r}
lr_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

lr_spec
```

We want to model the `Direction` of the stock market based on the percentage return from the previous 5 days plus the volume of the shares traded. `parsnip` requires that the response variable is factor, which is the case for this data set, so we don't need to do any change.

```{r}
lr_fit <- lr_spec %>%
    fit(Direction ~ . - Year - Today, data = Smarket)

lr_fit
```

The model's main info could be accessed with `summary`:

```{r}
summary(lr_fit$fit)
```

Or we can use the `tidy` function to extract some of these parameters.

```{r}
tidy(lr_fit)
```

Predictions are done the same way:

```{r}
predict(lr_fit, new_data = Smarket) %>% head()
```

The result is a tibble with a single column `.pred_class` which will be a factor variable of the same labels as the original training data set.  
We can also get back probability predictions, by specifying `type = "prob"`.

```{r}
predict(lr_fit, new_data = Smarket, type = 'prob') %>% head()
```

Using `augment` we can add the predictions to the data frame and then use that to look at model performance metrics. Before looking at the metrics, it is useful to look at the confusion matrix.

```{r}
augment(lr_fit, new_data = Smarket) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

A good performing model would have high numbers along the diagonal with small numbers on the off-diagonal. In this case, we see that the model isn't great, as it tends to predict *Down* as *Up* more often than it should.

If you want a visual representation of the confusion matrix, you can pipe the result to `ggplot2::autoplot`.

```{r, fig.align='center', out.width='80%'}
augment(lr_fit, new_data = Smarket) %>%
    conf_mat(truth = Direction, estimate = .pred_class) %>%
    autoplot(type = 'heatmap')
```

We can also calculate various performance metrics. One of the most common metrics is accuracy, which is how often the model predicted correctly as a percentage.

```{r}
augment(lr_fit, new_data = Smarket) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

To have more information about the model performance, let's split up the data, train it on some of it and then evaluate it on the remaining part. Since we are working with data that has a time component, it makes sense to fit the model using the first year's worth of data and evaluate it on the last year.

```{r}
Smarket_train <- Smarket %>% filter(Year != 2005)

Smarket_test <- Smarket %>% filter(Year == 2005)

# fit a logistic regression on the train data
lr_fit2 <- lr_spec %>%
    fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
        data = Smarket_train)

# Evaluate on the testing data set (confusion matrix)
augment(lr_fit2, new_data = Smarket_test) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

```{r}
# Evaluate on the testing data set (accuracy)
augment(lr_fit2, new_data = Smarket_test) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

We see that the model performs worse than the previous one.

Considering the logistic regression model had underwhelming p-values, let see what happens if we remove some of the variables that appear not to be helpful.

```{r}
lr_fit3 <- lr_spec %>%
    fit(Direction ~ Lag1 + Lag2, data = Smarket_train)

augment(lr_fit3, new_data = Smarket_test) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

```{r}
# Evaluate on the testing data set (accuracy)
augment(lr_fit3, new_data = Smarket_test) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

And we see an increase in performance. The model is still not perfect but it is starting to perform better.

### Linear Discriminant Analysis

We will perform LDA on the `Smarket` data, using the `parsnip::discrim_linear` function to create a LDA specification.

```{r}
# set model's specifications
lda_spec <- discrim_linear() %>%
    set_mode('classification') %>%
    set_engine('MASS')

# fit LDA model on train data
lda_fit <- lda_spec %>%
    fit(Direction ~ Lag1 + Lag2, data = Smarket_train)

lda_fit
```

One things to look for in the output is the group means. We see there is a slight difference between the means of the two groups, suggesting a tendency for the previous 2 days' returns to be negative on days when the market increases, and a tendency for the previous day' returns to be positive on days when the market declines.

Predictions are done just the same as with logistic regression

```{r}
predict(lda_fit, new_data = Smarket_test) %>% head()
```

```{r}
predict(lda_fit, new_data = Smarket_test, type = 'prob') %>% head()
```

And we can take a look at the performance

```{r}
augment(lda_fit, new_data = Smarket_test) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

```{r}
augment(lda_fit, new_data = Smarket_test) %>%
  accuracy(truth = Direction, estimate = .pred_class)
```

We see there isn't a markedly difference of performance between this model and the logistic regression.

### Quadratic Discriminant Analysis

This is the first time we try to fit a model that doesn't have a direct parsnip function. The QDA model is a special case of the regularized discriminant model. Setting `frac_common_cov = 0` and `frac_identity = 0` we are able to specify a QDA model.

```{r}
qda_spec <- 
    discrim_regularized(frac_common_cov = 0, frac_identity = 0) %>%
    set_mode('classification') %>%
    set_engine('klaR')

qda_fit <- qda_spec %>%
    fit(Direction ~ Lag1 + Lag2, data = Smarket_test)

qda_fit
```

```{r}
augment(qda_fit, new_data = Smarket_test) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

```{r}
augment(qda_fit, new_data = Smarket_test) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

In this case we see an increase in accuracy. However checking the confusion matrix we see that this model rarely predicts *'Down'*.

### K-Nearest Neighbors

This is the first model we have looked at that has a hyperparameter we need to specify. To begin let set it to 3 with `neighbors = 3`.

```{r}
knn_spec <- nearest_neighbor(neighbors = 3) %>%
    set_mode('classification') %>%
    set_engine('kknn')

knn_fit <- knn_spec %>%
    fit(Direction ~ Lag1 + Lag2, data = Smarket_train)

knn_fit
```

Evaluation is done the same way.

```{r}
augment(knn_fit, new_data = Smarket_test) %>%
    conf_mat(truth = Direction, estimate = .pred_class) 
```

```{r}
augment(knn_fit, new_data = Smarket_test) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

It appears that this model is not performing that well.

We will try using a KNN model in an application to caravan insurance data. This data set includes 85 predictors that measure demographic characteristics for 5822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.

We want to build a predictive model that uses the demographic characteristics to predict whether an individual is going to purchase a caravan insurance. Before we go on, we split the data set into a training data set and testing data set. (This is a not the proper way this should be done. See next chapter for the correct way.)

```{r}
Caravan_test  <- Caravan[seq_len(1000),]
Caravan_train <- Caravan[-seq_len(1000),]
```

Since we are using a KNN model, it is important that the variables are centered and scaled to make sure that the variables have a uniform influence. We can accomplish this transformation with `recipe::step_normalize`, which does it in one go.

```{r}
rec_spec <- recipe(Purchase ~ ., data = Caravan_train) %>%
    step_normalize(all_numeric_predictors())
```

We will be trying different values of K to see how the number of neighbors affect the model performance. A workflow object is created, with just the recipe added.

```{r}
Caravan_wf <- workflow() %>%
    add_recipe(rec_spec)

Caravan_wf
```

Next we create a general KNN model specification.

```{r}
knn_spec <- nearest_neighbor() %>%
    set_mode('classification') %>%
    set_engine('kknn')
```

We can use this model specification with the caravan workflow to create 3 full workflow objects for `K = 1,3,5`.

```{r}
knn1_wf <- Caravan_wf %>%
  add_model(knn_spec %>% set_args(neighbors = 1))

knn3_wf <- Caravan_wf %>%
  add_model(knn_spec %>% set_args(neighbors = 3))

knn5_wf <- Caravan_wf %>%
  add_model(knn_spec %>% set_args(neighbors = 5))
```

With all these workflow specification we can fit all the models one by one.

```{r}
knn1_fit <- fit(knn1_wf, data = Caravan_train)
knn3_fit <- fit(knn3_wf, data = Caravan_train)
knn5_fit <- fit(knn5_wf, data = Caravan_train)
```

And we can calculate all the confusion matricies.

```{r}
augment(knn1_fit, new_data = Caravan_test) %>%
    conf_mat(truth = Purchase, estimate = .pred_class)
```

```{r}
augment(knn3_fit, new_data = Caravan_test) %>%
    conf_mat(truth = Purchase, estimate = .pred_class)
```

```{r}
augment(knn5_fit, new_data = Caravan_test) %>%
    conf_mat(truth = Purchase, estimate = .pred_class)
```

### Comparing multiple models

We have fitted a lot of different models in this lab. And we were able to calculate the performance metrics one by one, but it is not ideal if we want to compare the different models. Below is an example of how you can more conveniently calculate performance metrics for multiple models at the same time.

Start of by creating a named list of the fitted models you want to evaluate. I have made sure only to include models that were fitted on the same parameters to make it easier to compare them.

```{r}
models <- list('logistic regression' = lr_fit3,
               'LDA'                 = lda_fit,
               'QDA'                 = qda_fit,
               'KNN'                 = knn_fit)
```

Next use `purrr::imap_dfr`to apply `augment` to each of the models using the testing data set. `.id = "model"` creates a column named "model" that is added to the resulting tibble using the names of models.

```{r}
preds <- purrr::imap_dfr(models, augment,
                         new_data = Smarket_test, .id = 'model')

preds %>%
    select(model, Direction, .pred_class, .pred_Down, .pred_Up) %>%
    head()
```

Yardstick provides many metrics, and we can combine them together with `metric_set`.

```{r}
multi_metric <- metric_set(accuracy, sensitivity, specificity)
```

and then the resulting function can be applied to calculate multiple metrics at the same time. All of the yardstick works with grouped tibbles so by calling `dplyr::group_by(model)` we can calculate the metrics for each of the models in one go.

```{r}
preds %>%
    group_by(model) %>%
    multi_metric(truth = Direction, estimate = .pred_class) %>%
    arrange(.metric, desc(.estimate))
```

The same technique can be used to create ROC curves.

```{r}
preds %>%
    group_by(model) %>%
    roc_curve(Direction, .pred_Down) %>%
    autoplot()
```

## Resampling Methods

